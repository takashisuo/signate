{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "\n",
    "import sys, sqlite3, os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "from collections import namedtuple\n",
    "import ipadic\n",
    "import MeCab\n",
    "from math import ceil\n",
    "import random\n",
    "\n",
    "# if you want to use chikkarpy library, remove comment\n",
    "#from chikkarpy import Chikkar\n",
    "#from chikkarpy.dictionarylib import Dictionary\n",
    "\n",
    "class EDA:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_synonyms(self, word):\n",
    "        pass\n",
    "\n",
    "    # 分かち書き\n",
    "    def wakati_text(self, text, hinshi=['名詞', '動詞']):\n",
    "        \"\"\"\n",
    "        分かち書き後のリストと同義語検索用の単語の原型リストを返す\n",
    "\n",
    "        Args:\n",
    "            text (str): 分かち書きする文章\n",
    "            hinshi (list, optional): 原型を取得する品詞. Defaults to ['名詞', '動詞'].\n",
    "\n",
    "        Returns:\n",
    "            list: 分かち書き後の単語、指定の品詞に絞った単語の原型リスト\n",
    "        \"\"\"\n",
    "        m = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
    "        p = m.parse(text)\n",
    "        p_split = [i.split(\"\\t\") for i in p.split(\"\\n\")][:-2]\n",
    "\n",
    "        # 原文の分かち書き\n",
    "        raw_words = [x[0] for x in p_split]\n",
    "\n",
    "        # 同義語検索用の単語の原型リスト（品詞を絞る）\n",
    "        second_half = [x[1].split(\",\") for x in p_split]\n",
    "        original_words = [x[6] if x[0] in hinshi else \"\" for x in second_half]\n",
    "        original_words = [\"\" if word in self.stop_words else word for word in original_words]\n",
    "\n",
    "        return raw_words, original_words\n",
    "\n",
    "    def synonym_replacement(self, raw_words, original_words, n):\n",
    "        \"\"\"\n",
    "        文章の単語をランダムにn個同義語で置き換える\n",
    "\n",
    "        Args:\n",
    "            raw_words (list): 分かち書き済みのリスト\n",
    "            original_words (list): 単語の原型のリスト\n",
    "            n (int): 単語を置き換える件数\n",
    "\n",
    "        Returns:\n",
    "            list\n",
    "        \"\"\"\n",
    "        new_words = raw_words.copy()\n",
    "\n",
    "        # 同義語に置き換える単語をランダムに決める\n",
    "        original_words_idx = [i for i, x in enumerate(original_words) if x != \"\"]\n",
    "        random.shuffle(original_words_idx)\n",
    "\n",
    "        # 指定の件数になるまで置き換え\n",
    "        num_replaced = 0\n",
    "        for idx in original_words_idx:\n",
    "            raw_word = raw_words[idx]\n",
    "            random_word = original_words[idx]\n",
    "            synonyms = self.get_synonyms(random_word)\n",
    "            if len(synonyms) >= 1:\n",
    "                synonym = random.choice(synonyms)\n",
    "                new_words = [synonym if word == raw_word else word for word in new_words]\n",
    "                num_replaced += 1\n",
    "            if num_replaced >= n:\n",
    "                break\n",
    "\n",
    "        return new_words\n",
    "\n",
    "    def random_insertion(self, raw_words, original_words, n):\n",
    "        \"\"\"\n",
    "        文章の中にランダムに単語をn個挿入\n",
    "\n",
    "        Args:\n",
    "            raw_words (list): 分かち書き済みのリスト\n",
    "            original_words (list): 単語の原型のリスト\n",
    "            n (int): 挿入する単語数\n",
    "\n",
    "        Returns:\n",
    "            list\n",
    "        \"\"\"\n",
    "        new_words = raw_words.copy()\n",
    "        for _ in range(n):\n",
    "            self.add_word(new_words, original_words)\n",
    "        return new_words\n",
    "\n",
    "    def add_word(self, new_words, original_words):\n",
    "        synonyms = []\n",
    "        counter = 0\n",
    "        insert_word_original = [x for x in original_words if x]\n",
    "        while len(synonyms) < 1:\n",
    "            random_word = insert_word_original[random.randint(0, len(insert_word_original)-1)]\n",
    "            synonyms = self.get_synonyms(random_word)\n",
    "            counter += 1\n",
    "            if counter >= 10:\n",
    "                return\n",
    "        random_synonym = synonyms[0]\n",
    "        random_idx = random.randint(0, len(new_words)-1)\n",
    "        new_words.insert(random_idx, random_synonym)\n",
    "\n",
    "\n",
    "    def random_deletion(self, words, p):\n",
    "        \"\"\"\n",
    "        文章の各単語を確率pで削除する\n",
    "\n",
    "        Args:\n",
    "            words (list): 分かち書き済みのリスト\n",
    "            p (float): 削除する確率\n",
    "\n",
    "        Returns:\n",
    "            list\n",
    "        \"\"\"\n",
    "        # 1文字しかなければ削除しない\n",
    "        if len(words) == 1:\n",
    "            return words\n",
    "\n",
    "        # 確率pでランダムに削除\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            r = random.uniform(0, 1)\n",
    "            if r > p:\n",
    "                new_words.append(word)\n",
    "\n",
    "        # 全て削除してしまったら、ランダムに1つ単語を返す\n",
    "        if len(new_words) == 0:\n",
    "            rand_int = random.randint(0, len(words)-1)\n",
    "            return [words[rand_int]]\n",
    "\n",
    "        return new_words\n",
    "\n",
    "    def random_swap(self, words, n):\n",
    "        \"\"\"\n",
    "        文章の単語の場所をn回入れ替える\n",
    "\n",
    "        Args:\n",
    "            words (list): 分かち書き済みのリスト\n",
    "            n (int): 入れ替える回数\n",
    "\n",
    "        Returns:\n",
    "            list\n",
    "        \"\"\"\n",
    "        new_words = words.copy()\n",
    "        for _ in range(n):\n",
    "            nwords = self.swap_word(new_words)\n",
    "\n",
    "        return new_words\n",
    "\n",
    "    def swap_word(self, new_words):\n",
    "        random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "        random_idx_2 = random_idx_1\n",
    "        counter = 0\n",
    "        while random_idx_2 == random_idx_1:\n",
    "            random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "            counter += 1\n",
    "            if counter > 3:\n",
    "                return new_words\n",
    "        new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "\n",
    "        return new_words\n",
    "\n",
    "    # 各手法をまとめて実行\n",
    "    def eda(self, sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "        \"\"\"\n",
    "        EDAの各手法をまとめて実行して、指定の件数分のEDA済み類似文章+原文をリストで返す。\n",
    "        原文はリストの最後に挿入される。\n",
    "\n",
    "        Args:\n",
    "            sentence (str): EDAを実行する文章（原文）\n",
    "            alpha_sr (float, optional): synonym_replacementのalpha. Defaults to 0.1.\n",
    "            alpha_ri (float, optional): random_insertionのalpha. Defaults to 0.1.\n",
    "            alpha_rs (float, optional): random_swapのalpha. Defaults to 0.1.\n",
    "            p_rd (float, optional): random_deletionのalpha. Defaults to 0.1.\n",
    "            num_aug (int, optional): EDAで作成する文章数. Defaults to 9.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "\n",
    "        # 分かち書き\n",
    "        raw_words, original_words = self.wakati_text(sentence)\n",
    "        num_words = len(raw_words)\n",
    "\n",
    "        augmented_sentences = []\n",
    "        techniques = ceil(alpha_sr) + ceil(alpha_ri) + ceil(alpha_rs) + ceil(p_rd)\n",
    "        if techniques == 0:\n",
    "            return\n",
    "\n",
    "        num_new_per_technique = int(num_aug/techniques)+1\n",
    "\n",
    "        #ランダムに単語を同義語でn個置き換える\n",
    "        if (alpha_sr > 0):\n",
    "            n_sr = max(1, int(alpha_sr*num_words))\n",
    "            for _ in range(num_new_per_technique):\n",
    "                a_words = self.synonym_replacement(raw_words,original_words ,n_sr)\n",
    "                augmented_sentences.append(''.join(a_words))\n",
    "\n",
    "        #ランダムに文中に出現する単語の同義語をn個挿入\n",
    "        if (alpha_ri > 0):\n",
    "            n_ri = max(1, int(alpha_ri*num_words))\n",
    "            for _ in range(num_new_per_technique):\n",
    "                a_words = self.random_insertion(raw_words,original_words, n_ri)\n",
    "                augmented_sentences.append(''.join(a_words))\n",
    "\n",
    "        #ランダムに単語の場所をn回入れ替える\n",
    "        if (alpha_rs > 0):\n",
    "            n_rs = max(1, int(alpha_rs*num_words))\n",
    "            for _ in range(num_new_per_technique):\n",
    "                a_words = self.random_swap(raw_words, n_rs)\n",
    "                augmented_sentences.append(''.join(a_words))\n",
    "\n",
    "        #ランダムに単語を確率pで削除する\n",
    "        if (p_rd > 0):\n",
    "            for _ in range(num_new_per_technique):\n",
    "                a_words = self.random_deletion(raw_words, p_rd)\n",
    "                augmented_sentences.append(''.join(a_words))\n",
    "\n",
    "        #必要な文章の数だけランダムに抽出\n",
    "        random.shuffle(augmented_sentences)\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "\n",
    "        #原文もリストに加える\n",
    "        augmented_sentences.append(sentence)\n",
    "\n",
    "        return augmented_sentences\n",
    "\n",
    "\n",
    "class WordnetEDA(EDA):\n",
    "\n",
    "    def __init__(self):\n",
    "        ROOT: str = 'Z:\\\\' if os.name == 'nt' else '/Z'\n",
    "        RESOURCE_ROOT: str = os.path.join(ROOT, 'ae_share02', '128_TotalLifePlanningSalesTraining', 'smartphone_nlp', 'dev', 'resources')\n",
    "        WORDNET_PATH: str = os.path.join(RESOURCE_ROOT, \"synonym\", \"wn_jpn.db\")\n",
    "        STOPWORDS_PATH: str = os.path.join(RESOURCE_ROOT, \"stopwords\", \"slothlib_stopwords.txt\")\n",
    "        # synset(概念ID)とlemma(単語)の組み合わせDataFrameの作成\n",
    "        conn = sqlite3.connect(WORDNET_PATH)\n",
    "        q = 'SELECT synset,lemma FROM sense,word USING (wordid) WHERE sense.lang=\"jpn\"'\n",
    "        self.sense_word = pd.read_sql(q, conn)\n",
    "        # stop words\n",
    "        self.stop_words = pd.read_csv(STOPWORDS_PATH, header=None)[0].to_list()\n",
    "\n",
    "    # 類義語をリストにして返す\n",
    "    def get_synonyms(self, word):\n",
    "        \"\"\"\n",
    "        入力した単語の類似語を日本語wordnetから検索してリスト化\n",
    "\n",
    "        Args:\n",
    "            word (str): 類似語を検索する単語\n",
    "\n",
    "        Returns:\n",
    "            list: 入力した単語の類似語\n",
    "        \"\"\"\n",
    "        synsets = self.sense_word.loc[self.sense_word.lemma == word, \"synset\"]\n",
    "        synset_words = set(self.sense_word.loc[self.sense_word.synset.isin(synsets), \"lemma\"])\n",
    "\n",
    "        if word in synset_words:\n",
    "            synset_words.remove(word)\n",
    "\n",
    "        return list(synset_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SudachiEDA(EDA):\n",
    "\n",
    "    def __init__(self):\n",
    "        ROOT:str = 'Z:\\\\' if os.name == 'nt' else '/Z'\n",
    "        PROJ_ROOT:str = os.path.join(ROOT, 'ae_share02', '128_TotalLifePlanningSalesTraining', 'smartphone_nlp', 'poc')\n",
    "        SYNONYM_FILE = os.path.join(PROJ_ROOT, 'resources', 'synonym', 'synonyms.txt')\n",
    "        self.df = pd.read_csv(SYNONYM_FILE, skip_blank_lines=True,\n",
    "                            names=('group_id', 'type', 'expand', 'vocab_id', 'relation', 'abbreviation', 'spelling', 'domain', 'surface', 'reserve1', 'reserve2'))\n",
    "        RESOURCE_ROOT: str = os.path.join(ROOT, 'ae_share02', '128_TotalLifePlanningSalesTraining', 'smartphone_nlp', 'poc', 'resources')\n",
    "        STOPWORDS_PATH: str = os.path.join(RESOURCE_ROOT, \"stopwords\", \"slothlib_stopwords.txt\")\n",
    "        # stop words\n",
    "        self.stop_words = pd.read_csv(STOPWORDS_PATH, header=None)[0].to_list()\n",
    "\n",
    "    def get_synonyms(self, word:str)->list:\n",
    "        \"\"\"get synonyms using sudachi-dictionary\n",
    "\n",
    "        Args:\n",
    "            word (str): The word you want to search\n",
    "\n",
    "        Returns:\n",
    "            list: exclude specified word\n",
    "        \"\"\"\n",
    "        cols = ['group_id', 'surface']\n",
    "        all_df = pd.DataFrame(columns=cols)\n",
    "        for row in self.df[self.df.surface==word].itertuples():\n",
    "            all_df = pd.concat([all_df, self.df[self.df.group_id==row.group_id].loc[:, cols]])\n",
    "        surfaces: list = all_df['surface'].tolist()\n",
    "        ex_surfaces = [n for n in surfaces if word != n]\n",
    "        return ex_surfaces\n",
    "\n",
    "class WordnetProxy:\n",
    "\n",
    "    def __init__(self):\n",
    "        ROOT:str = 'Z:\\\\' if os.name == 'nt' else '/Z'\n",
    "        PROJ_ROOT:str = os.path.join(ROOT, 'ae_share02', '128_TotalLifePlanningSalesTraining', 'smartphone_nlp', 'poc')\n",
    "        self.conn = sqlite3.connect(os.path.join(PROJ_ROOT, 'resources', 'synonym', 'wn_jpn.db'))\n",
    "\n",
    "    def __getWords(self, lemma):\n",
    "        Word = namedtuple('Word', 'wordid lang lemma pron pos')\n",
    "        cur = self.conn.execute(\"select * from word where lemma=?\", (lemma,))\n",
    "        return [Word(*row) for row in cur]\n",
    "\n",
    "    def __getSenses(self, word):\n",
    "        Sense = namedtuple('Sense', 'synset wordid lang rank lexid freq src')\n",
    "        cur = self.conn.execute(\"select * from sense where wordid=?\", (word.wordid,))\n",
    "        return [Sense(*row) for row in cur]\n",
    "\n",
    "    def __getSynset(self, synset):\n",
    "        Synset = namedtuple('Synset', 'synset pos name src')\n",
    "        cur = self.conn.execute(\"select * from synset where synset=?\", (synset,))\n",
    "        return Synset(*cur.fetchone())\n",
    "\n",
    "    def __getWordsFromSynset(self, synset, lang):\n",
    "        Word = namedtuple('Word', 'wordid lang lemma pron pos')\n",
    "        cur = self.conn.execute(\"select word.* from sense, word where synset=? and word.lang=? and sense.wordid = word.wordid;\", (synset,lang))\n",
    "        return [Word(*row) for row in cur]\n",
    "\n",
    "    def __getWordsFromSenses(self, sense, lang=\"jpn\"):\n",
    "        synonym = {}\n",
    "        for s in sense:\n",
    "            lemmas = []\n",
    "            syns = self.__getWordsFromSynset(s.synset, lang)\n",
    "            for sy in syns:\n",
    "                lemmas.append(sy.lemma)\n",
    "            synonym[self.__getSynset(s.synset).name] = lemmas\n",
    "        return synonym\n",
    "\n",
    "    def get_synonym(self, word:str)->dict:\n",
    "        \"\"\"get synonyms using wordnet DB\n",
    "\n",
    "        Args:\n",
    "            word (str):The word you want to search\n",
    "\n",
    "        Raises:\n",
    "            Exception: if word does not string type\n",
    "\n",
    "        Returns:\n",
    "            dict: KEY:english related word, VALUE:japanese word.\n",
    "        \"\"\"\n",
    "        synonym = {}\n",
    "        if type(word) is not str:\n",
    "            raise Exception(f'accept only string. current variable type is {type(word)}.')\n",
    "\n",
    "        words = self.__getWords(word)\n",
    "        if words:\n",
    "            for w in words:\n",
    "                sense = self.__getSenses(w)\n",
    "                s = self.__getWordsFromSenses(sense)\n",
    "                synonym = dict(list(synonym.items()) + list(s.items()))\n",
    "        return synonym"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "883087307fe87681cbf47ee0229b5cd42f863d1d1e299ac01a4c72e5e7e45007"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
